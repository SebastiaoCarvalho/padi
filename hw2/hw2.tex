\documentclass{article}

\usepackage{amsmath}
\usepackage{kbordermatrix}% https://kcborder.caltech.edu/TeX/kbordermatrix.sty
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
\renewcommand{\thesubsection}{\thesection \space \alph{subsection})}
\renewcommand{\kbldelim}{[}% Left delimiter
\renewcommand{\kbrdelim}{]}% Right delimiter

\title{Planing, Learning and Intelligent Decision Making - Homework 2}
\author{99326 - Sebasti√£o Carvalho, 99331 - Tiago Antunes}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\section{Question 1}

\subsection{}

Considering $\mathcal{X}$ as our state space, $\mathcal{X} = \{1P, 2P, 2NP, 3P, 3NP, 4P, 4NP\}$, 
where each state represents the corresponding cell and if they have the passanger or not (P or NP). 
Since when we are on the first cell we always have the passanger, we dont consider state $1NP$.

Considering $\mathcal{A}$ as our action space, $\mathcal{A} = \{U, D, L, R\}$, 
where each action represents the movement of the agent, up, down, left and right, respectively.

\subsection{}

The transition matrix for the action down, $P_D$, is given by
$
    \kbordermatrix{
    & 1P & 2P & 2NP & 3P & 3NP & 4P & 4NP \\
    1P & 0.2 & 0 & 0 & 0.8 & 0 & 0 & 0 \\
    2P & 0 & 0.2 & 0 & 0 & 0 & 0.8 & 0 \\
    2NP & 0 & 0 & 0.2 & 0 & 0 & 0 & 0.8 \\
    3P & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
    3NP & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
    4P & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    4NP & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
  }
$

\newpage

Since we want a simple cost function, we'll consider that the cost for every action is 1, 
except for the actions from the goal state, which will have a cost of 0.

\medskip

So, the cost matrix for all actions is given by 
$
  C = \kbordermatrix{
    & U & D & L & R \\
    1P & 1 & 1 & 1 & 1 \\
    2P & 0 & 0 & 0 & 0 \\
    2NP & 1 & 1 & 1 & 1 \\
    3P & 1 & 1 & 1 & 1 \\
    3NP & 1 & 1 & 1 & 1 \\
    4P & 1 & 1 & 1 & 1 \\
    4NP & 1 & 1 & 1 & 1 \\
  }
$

\medskip

Basically, $c(x, a) = \left\{\begin{array}{ll}
    0 & \text{if } x = 2P \\
    1 & \text{else}
\end{array}
\right.$

\subsection{}

Let policy, $\pi$, be the policy in which the taxi driver always goes down. In this case there's only one action the taxi can do, action 'D',
therefore the policy matrix can be represented by: 

\begin{equation}
    \notag
    \pi = \begin{bmatrix}
        0 & 1 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
    \end{bmatrix}
\end{equation}

The policy is Markov and stationary, since the distribution over action given the history depends only on the last state and
doesn't change through time. \\

Because the policy is stationary, the cost-to-go function associated with it, verifies:
\begin{equation}
    \notag
    J^\pi = c_\pi + {\gamma}P_{\pi}J^{\pi}
\end{equation}

Solving the system, we get:
\begin{equation}
    \notag
    J^\pi = {(I - {\gamma}P_{\pi})^{-1}}c_\pi
\end{equation}

\medskip

The cost matrix for the policy $\pi$ can be obtained by multiplying the policy matrix with the immediate cost matrix value by value, and then summing the values in each row.
Since here all columns in $\pi$ are zeros, except the column corresponding to action 'D', we only need to calculate for that action, since all else will simply be 0. Here we put the column corresponding to action 'D' in the policy matrix in a diagonal and multiply both matrices to get the result.

\begin{equation}
    \notag
    c_{\pi} = diag(\pi(D|\cdot))C(D|\cdot)
    \begin{bmatrix}
        1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 1 \\
    \end{bmatrix}
    \begin{bmatrix}
        1 \\
        0 \\
        1 \\
        1 \\
        1 \\
        1 \\
        1 \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        1 \\
        0 \\
        1 \\
        1 \\
        1 \\
        1 \\
        1 \\
    \end{bmatrix}
\end{equation}

\bigskip

Now we will calculate the matrix $P_{\pi}$.
This matrix is obtained by multiplying each value of a column by each row of the transition probability matrix of the action the column corresponds to.
In other words, the probability of doing an action in a state by the transition probabilities of that state, and then summing the matrices corresponding results for each action.
Since here all columns in $\pi$ are zeros, except the column corresponding to action 'D', we only need to calculate for that action, since all else will simply be 0.
Here, once again, we put the column corresponding to action 'D' in the policy matrix in a diagonal and multiply both matrices to get the result.

\begin{equation}
    \notag
    P_{\pi} = diag(\pi(D|\cdot))P_{D} = 
    \begin{bmatrix}
        1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 1 \\
    \end{bmatrix}
    \begin{bmatrix}
        0.2 & 0 & 0 & 0.8 & 0 & 0 & 0 \\
        0 & 0.2 & 0 & 0 & 0 & 0.8 & 0 \\
        0 & 0 & 0.2 & 0 & 0 & 0 & 0.8 \\
        0 & 0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 1 \\
    \end{bmatrix}
\end{equation}

\begin{equation}
    \notag
    =
    \begin{bmatrix}
        0.2 & 0 & 0 & 0.8 & 0 & 0 & 0 \\
        0 & 0.2 & 0 & 0 & 0 & 0.8 & 0 \\
        0 & 0 & 0.2 & 0 & 0 & 0 & 0.8 \\
        0 & 0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 1 \\
    \end{bmatrix}
\end{equation}
\medskip

Now, we have everything we need to calculate $J_{\pi}$. Considering $\gamma = 0.9$ we have:

\begin{equation}
    \notag
    J^\pi = \left(
    \begin{bmatrix}
        1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 1 \\
    \end{bmatrix} - 0.9
    \begin{bmatrix}
        0.2 & 0 & 0 & 0.8 & 0 & 0 & 0 \\
        0 & 0.2 & 0 & 0 & 0 & 0.8 & 0 \\
        0 & 0 & 0.2 & 0 & 0 & 0 & 0.8 \\
        0 & 0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 1 \\
    \end{bmatrix}\right)^{-1}
    \begin{bmatrix}
        1 \\
        0 \\
        1 \\
        1 \\
        1 \\
        1 \\
        1 \\
    \end{bmatrix}
\end{equation}

\begin{equation}
    \notag
    = 
    \begin{bmatrix}
        1.22 & 0 & 0 & 8.78 & 0 & 0 & 0 \\ 
        0 & 1.22 & 0 & 0 & 0 & 8.78 & 0 \\ 
        0 & 0 & 1.22 & 0 & 0 & 0 & 8.78 \\
        0 & 0 & 0 & 10 & 0 & 0 & 0 \\  
        0 & 0 & 0 & 0 & 10 & 0 & 0 \\ 
        0 & 0 & 0 & 0 & 0 & 10 & 0 \\  
        0 & 0.& 0 & 0 & 0 & 0 & 10 \\
    \end{bmatrix}
    \begin{bmatrix}
        1 \\
        0 \\
        1 \\
        1 \\
        1 \\
        1 \\
        1 \\
    \end{bmatrix} = 
    \begin{bmatrix}
        10 \\
        8.78 \\
        10 \\
        10 \\
        10 \\
        10 \\
        10 \\
    \end{bmatrix}
\end{equation}

\medskip

So, the cost-to-go function associated with policy $\pi$, using $\gamma = 0.9$, is: 

\begin{equation}
    \notag
    J^\pi = 
    \kbordermatrix{
    & \\
    1P & 10 \\
    2P & 8.78 \\
    2NP & 10 \\
    3P & 10 \\
    3NP & 10 \\
    4P & 10 \\
    4NP & 10 \\
    }
\end{equation}

\end{document}